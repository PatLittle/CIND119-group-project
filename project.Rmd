---
title: "CIND119 - Group Project"
date: "10/04/2021"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)
library(tidymodels)
library(vip)
library(rpart.plot)
library(DataExplorer)
library(tictoc)
library(data.table)
library(gtools)
```

## Memebers

Patrick Litte, patrick.little@ryerson.ca <br>
Manjola Chiappetta, m1chiappetta@ryerson.ca

## Summary

Our client for this project is a Portuguese bank that is looking for improvements to their telemarketing strategy.  This bank is marketing long-term deposit accounts such as bonds and savings account to a large variety of existing clients with a wide range of attributes. Our role in the project was to create a variety of predictive models which could be used to predict whether a client will subscribe to a term deposit or not, based on the bank’s past experience in marketing these products.  This predictive model would enable the client to target their marketing activities more efficiently to clients with a greater likelihood of subscribing to a term deposit.
<br>
In this project we compared two different predictive models, decision tree and Naïve Bayes and compared their accuracy as well as their Area under the Receiver Operator Curve.  For our extra credit, we compared these two more simple models against a more advanced Gradient Boosting model (XGBoost) to determine if a more advanced model could out preform the simpler models. 
<br>
###Tools
This project will leverage R for the exploratory data analysis, data preprocessing, predictive modeling, and model evaluation.  We will use the tidymodels framework to implement our predictive modelling, and model evaluation, and we will use the DataExplorer package to aid in exploratory data analysis.   


## Workload Distribution

| Member Name        | List of Tasks Preformed |
|--------------------|-------------------------|
| Patrick Little     | - some tasks            |
| Manjola Chiappetta | - some tasks            |

## Exploratory Data Analysis

In this section we will:
  - Look at the attribute types in the dataset
  - Find and missing values
  - Examine the pairwise association between our variables
  - Find max,min,mean and standard deviation of the atttributes
  - Determine any outlier values for the attributes under consideration
  - Analyze the distribution of numeric attributes

To begin our exploratory data analysis we can use the data explorer R package to generate an easy to consume overview of our dataset. In this plot one can observe that there are a mix of discrete and continuous variables in our dataset. Additionally we can observe that there is no missing data observations contained within our dataset. 

```{r}
bank<-read.csv("https://raw.githubusercontent.com/PatLittle/CIND119-group-project/main/bank_marketing/bank.csv")
plot_intro(bank)
```
Looking at the individual columns of our dataset, one can observe their are 17 variables in our dataset. Age, balance, day, duration, campaign, duration, pdays, and previous are of type integer. Job, marital, education, default, housing, loan, contact, month, poutcome, and y are of type character. 

```{r}
plot_str(bank, type="d")
```
<br>
We can examine the values of our character variables to determine if character is the appropriate data type for each of the variables. If a variable of type character contains a limited number of possible values, then converting that variable to be of type factor will allow our predictive models to interpret that data without significant additional prepossessing. If those variables contain free form text without a controlled vocabulary for that variable, then likely additional natural language processing techniques such as stemming and lemmatizaion, along with other preprocessing or feature engineering may be required before those variables would offer significant predictive value to our models. 

```{r}

job<-levels(as.factor(bank$job))
marital<-levels(as.factor(bank$marital))
education<-levels(as.factor(bank$education))
default<-levels(as.factor(bank$default))
housing<-levels(as.factor(bank$housing))
loan<-levels(as.factor(bank$loan))
contact<-levels(as.factor(bank$contact))
month<-levels(as.factor(bank$month))
poutcome<-levels(as.factor(bank$poutcome))
y<-levels(as.factor(bank$y))

na.pad <- function(x,len){
    x[1:len]
}

makePaddedDataFrame <- function(l,...){
    maxlen <- max(sapply(l,length))
    data.frame(lapply(l,na.pad,len=maxlen),...)
}

a = job
b = marital
c = education
d = default
e = loan
f = contact
g = month
h = poutcome
j = y

data_dict<-makePaddedDataFrame(list("Job"=a,"Marital"=b,"Education"=c,"Default"=d,"Loan"=e,"Contact"=f,"Month"=g,"pOutcome"=h,"y"=j))

data_dict %>% na.replace("")
options(knitr.kable.NA = '')
kable(data_dict, caption="Character Variables with all Existing Values")
```
Examining our table above of each of the variables of type character with the universe of existing values for each variables, serving as a simple data dictionary, we can determine that this is a controlled vocabulary and there is not unstructured text in these fields. Since there is a controlled vocabulary being used with these variables, we will convert these variables to be of type factor.

```{r}
bank_clean<- bank %>% mutate_if(is.character, factor)
```

<br>

We can then examine the pairwise association between all of the variables in the dataframe. In this analysis we will use Spearman Correlation to measure the association between our numeric variables, Cramer's V for our nominal factor data, and ANOVA to compare numeric data with factor data. 


```{r}



require(rcompanion)


# Calculate a pairwise association between all variables in a data-frame. In particular nominal vs nominal with Chi-square, numeric vs numeric with Spearman correlation, and nominal vs numeric with ANOVA.
# Adopted from https://stackoverflow.com/a/52557631/590437
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
    df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")

    is_nominal = function(x) class(x) %in% c("factor", "character")
    # https://community.rstudio.com/t/why-is-purr-is-numeric-deprecated/3559
    # https://github.com/r-lib/rlang/issues/781
    is_numeric <- function(x) { is.integer(x) || is_double(x)}

    f = function(xName,yName) {
        x =  pull(df, xName)
        y =  pull(df, yName)

        result = if(is_nominal(x) && is_nominal(y)){
            # use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
            cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
            data.frame(xName, yName, assoc=cv, type="cramersV")

        }else if(is_numeric(x) && is_numeric(y)){
            correlation = cor(x, y, method=cor_method, use="complete.obs")
            data.frame(xName, yName, assoc=correlation, type="correlation")

        }else if(is_numeric(x) && is_nominal(y)){
            # from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
            r_squared = summary(lm(x ~ y))$r.squared
            data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")

        }else if(is_nominal(x) && is_numeric(y)){
            r_squared = summary(lm(y ~x))$r.squared
            data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")

        }else {
            warning(paste("unmatched column type combination: ", class(x), class(y)))
        }

        # finally add complete obs number and ratio to table
        result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
    }

    # apply function to each variable combination
    map2_df(df_comb$X1, df_comb$X2, f)
}

cor_data<-mixed_assoc(bank_clean) %>% subset(assoc<0.99999) %>% arrange(desc(abs(assoc)))
kable(cor_data[1:16,1:4],row.names = F, caption = "Mixed Data Types Correlation/Association")
```
Looking at the top 8 pairs of variables in our pairwise comparison sorted by the absolute value of the association measure, we can see that the variables Previous and pDays seem to have a high measure of correlation. With a Spearman value of 0.986, we can state there is a highly monotonic relation between the variables. In this metric we are assessing if the variables move in the same direction, however this relation could be have one of a number of non-linear relation types such as exponential, logistic, etc.

Looking at the relations between our numeric variables with a different metric, one can use the Pearson correlation to assess the linearity of the relationships. With an R value of 0.58, the relation between the pDays and Previous variables is a moderate positive correlation. Since the R value is in a moderate range, there is still likely predictive value in including both variables in our models.

```{r}
plot_correlation(bank_clean, type = "c", cor_args = list("method"="pearson"))
```
We can then look for outliers in our numeric variables. 

```{r}

kable(summary(bank_clean))

```



## Predictive Modeling / Classification

### Decision Tree

```{r}

###Decision Tree





set.seed(888)
bank_split <- initial_split(bank_clean, prop = 0.75, 
                             strata = y)

bank_training <- bank_split %>% training()
bank_test <- bank_split %>% testing()
bank_folds <- vfold_cv(bank_training, v = 10)



bank_recipe <- recipe(y ~ ., data = bank_training) 
 


bank_clean_baked<-bank_recipe %>% 
  prep() %>% 
  bake(new_data = bank_training)

tree_model <- decision_tree(cost_complexity = tune(),
                            tree_depth = tune(),
                            min_n = tune()) %>% 
  set_engine('rpart') %>% 
  set_mode('classification')

tree_workflow <- workflow() %>% 
  add_model(tree_model) %>% 
  add_recipe(bank_recipe)

tree_grid <- grid_latin_hypercube(cost_complexity(),
                          tree_depth(),
                          min_n(), 
                          size = 60)

set.seed(888)

tree_tuning <- tree_workflow %>% 
  tune_grid(resamples = bank_folds,
            grid = tree_grid)

tree_tuning %>% show_best('roc_auc')

best_tree <- tree_tuning %>% 
  select_best(metric = 'roc_auc')


final_tree_workflow <- tree_workflow %>% 
  finalize_workflow(best_tree)


tree_wf_fit <- final_tree_workflow %>% 
  fit(data = bank_training)

tree_fit <- tree_wf_fit %>% 
  pull_workflow_fit()

vip(tree_fit)

rpart.plot(tree_fit$fit, roundint = FALSE)

tree_last_fit <- final_tree_workflow %>% 
  last_fit(bank_split)

tree_last_fit %>% collect_metrics()


tree_last_fit %>% collect_predictions() %>% 
  roc_curve(truth  = y, estimate = .pred_no) %>% 
  autoplot()

tree_predictions <- tree_last_fit %>% collect_predictions()

conf_mat(tree_predictions, truth = y, estimate = .pred_class)

predict(tree_last_fit$.workflow[[1]],bank_test[15,])

saveRDS(tree_last_fit$.workflow[[1]],"./saved_model.Rds")

trained_model<-readRDS("saved_model.Rds")

```
### Naive Bayes

```{r}



set.seed(888)
nb_split <- initial_split(bank_clean, prop = 0.75, 
                            strata = y)

nb_training <- nb_split %>% training()
nb_test <- nb_split %>% testing()
nb_folds <- vfold_cv(nb_training, v = 10)

nb_recipe <- recipe(y ~ ., data = nb_training)
  


nb_wf <- workflow() %>%
  add_recipe(nb_recipe)

library(discrim)
nb_spec <- naive_Bayes() %>%
  set_mode("classification") %>%
  set_engine("naivebayes")

nb_spec

nb_fit <- nb_wf %>%
  add_model(nb_spec) %>%
  fit(data = nb_training)

nb_wf_final <-  workflow() %>%
  add_recipe(nb_recipe) %>%
  add_model(nb_spec)

nb_rs <- fit_resamples(
  nb_wf_final,
  nb_folds,
  control = control_resamples(save_pred = TRUE)
)


nb_last_fit <- nb_wf_final %>% 
  last_fit(nb_split)

nb_last_fit %>% collect_metrics()

nb_last_fit %>% collect_predictions() %>% 
  roc_curve(truth  = y, estimate = .pred_no) %>% 
  autoplot()

nb_predictions <- nb_last_fit %>% collect_predictions()
conf_mat(nb_predictions, truth = y, estimate = .pred_class)



```
## XGBoost

```{r}

as.data.table(bank_clean)

set.seed(888)
xg_split<- initial_split(bank_clean)
xg_train<-training(xg_split)
xg_test<-testing(xg_split)

set.seed(888)
xg_folds<-vfold_cv(xg_train,v=10)

xgb_spec <- boost_tree(
  trees = 1000, 
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     
  sample_size = tune(), mtry = tune(),         
  learn_rate = tune()                        
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), xg_train),
  learn_rate(),
  size = 60
)

xgb_grid

xgb_recipe <- recipe(y ~ ., data = xg_train) %>%
  step_dummy(all_nominal(), -all_outcomes())
  


tic()
xgb_recipe %>%
  prep() %>%
  bake(new_data = xg_train) 
toc()


xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(xgb_recipe)

xgb_wf


library(doParallel)
cores<-detectCores()
cl<- makeCluster(cores[1]-4)
registerDoParallel(cl)

tic()
set.seed(888)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = xg_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE))
toc()

best_auc <- select_best(xgb_res, "roc_auc")
best_auc


final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

tic()
final_res <- last_fit(final_xgb, xg_split)
collect_metrics(final_res)
toc()

final_res %>%
  collect_predictions() %>%
  roc_curve(y, .pred_no) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )


final_res %>%
  collect_predictions() %>% 
  conf_mat(truth = y, estimate = .pred_class)

library(vip)
final_xgb %>%
  fit(data = xg_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")

final_res %>% collect_metrics()
 ```


## Conclusions and Recommendations

Some text wrapping up the report